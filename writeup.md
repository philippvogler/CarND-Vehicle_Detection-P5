# Vehicle Detection Project

The steps of this project are the following:

* Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier
* Optionally, you can also apply a color transform and append binned color features, as well as histograms of color, to your HOG feature vector.
* Note: for those first two steps don't forget to normalize your features and randomize a selection for training and testing.
* Implement a sliding-window technique and use your trained classifier to search for vehicles in images.
* Run your pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.
* Estimate a bounding box for vehicles detected.

[//]: # (Image References)
[image1]: ./output_images/car_not_car.png
[image2]: ./output_images/HOG_example.png
[image3]: ./output_images/sliding_window.png
[image4]: ./output_images/sliding_windows.png
[image5]: ./output_images/bboxes_and_heat.png
[image6]: ./output_images/labels_map.png
[image7]: ./output_images/output_bboxes.png
[video1]: ./output_video.mp4

## [Rubric](https://review.udacity.com/#!/rubrics/513/view) Points
### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  

---
### Writeup

### Histogram of Oriented Gradients (HOG)

#### Explain how (and identify where in your code) you extracted HOG features from the training images.

The code for this step is contained in code cells 3, to 10 of the IPython notebook 'Vehicle detection project'.  

I started by reading in all the `vehicle` and `non-vehicle` images (code cell 2).  Here is an example of one of each of the `vehicle` and `non-vehicle` classes (code cell 9):

![alt text][image1]

After copying in the feature extraction functioons from the lessons (code cell 3), I explored different color spaces and different `skimage.hog()` parameters (`orientations`, `pixels_per_cell`, and `cells_per_block`).  I grabbed random images from each of the two classes and displayed them to get a feel for what the `skimage.hog()` output looks like (code cell 10).

Here is an example using the `RGB` color space and HOG parameters of `orientations=6`, `pixels_per_cell=(8, 8)` and `cells_per_block=(2, 2)`:

![alt text][image2]

#### 2. Explain how you settled on your final choice of HOG parameters.

I sticked closely with the recommendations from Ryan's Q and A video. I improved the choices further with the accuracy results of the classifier (see below; code cell 11). I also looked at the results of the first application to images (code cell 12).

Optimizing only based on the plots of the HOG images was kind of useless. I discarded most of the changes I made.

#### 3. Describe how (and identify where in your code) you trained a classifier using your selected HOG features (and color features if you used them).

I trained a linear SVM using the hole data set im code cell 11. In addition to choosing all HOG channels I increased the orient feature. I terms of color features I - like recommended by Ryan - choose the 'YCrCb' color space. I also increased the number of histogram bins.
The feature vectors got stacked and scaled using 'np.vstack' and 'StandardScaler()'. Training and test set were randomly generated by 'train_test_split()'.

### Sliding Window Search

#### 1. Describe how (and identify where in your code) you implemented a sliding window search.  How did you decide what scales to search and how much to overlap windows?

I cell 12 I coded up my sliding window search. I started out with the impression that it would make sense to scale the sliding windows as small as possible to be able to identify cars in the distance. I overshot it with windows size 64 by 64. This window size produced plenty false positives. I settled on window size 96x96 and 128x128. Theses two sizes detected all the cars in the test images with very few false positives.
Increasing the overlap to 75% increased the accuracy to the detections.
Limiting the search to the area of interest (street) on the y axis also helps to avoid false positives in the sky and the trees. This limited the number of windows and processing time as well.

![alt text][image3]

#### 2. Show some examples of test images to demonstrate how your pipeline is working.  What did you do to optimize the performance of your classifier?

To improve the results I made changes to the classifier and the sliding window procedure:

First I tweaked the different feature extractors to improve the classifier (code cell 11). I...

* switched to the YCrCb color space as recommended in the Q&A.
* increased the orient of the hog feature extractor to 9.
* used all HOG channels.
* lowered the spatial_size to 16,16.
* dropped the color histograms.

To prevent overfitting I added a grid search with three fold cross-validation for the regularization parameter C. I reached an accuracy score of 99% with a C value of 0.3.

Than I changed the sliding window procedure (code cell 12) by
cropping the area of interest to 400, 656 on the y axis. I searched with two window sizes (96x96 and 128x128) to detect vehicles close and far away and increased the overlap to 75% for more spot on detection, as described above.

These changes reduced false positives significantly.

![alt text][image4]
---

### Video Implementation

#### Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (somewhat wobbly or unstable bounding boxes are ok as long as you are identifying the vehicles most of the time with minimal false positives.)
Here's a [link to my video result](./output_video.mp4)


#### Describe how (and identify where in your code) you implemented some kind of filter for false positives and some method for combining overlapping bounding boxes.

I recorded the positions of positive detections in each frame of the video. From the positive detections I created heat maps to amplify regions with multiple detection (find_cars function in code cell 15). The images were searched at four different scales: 0.8, 1.0, 1.5, and 2.0 to find cars of all possible sizes. Then I put these heat maps in a queue to sum up the most recent 20 heat maps. This heat map sum was thresholded to correctly identify vehicle positions and not some random blips in single heat maps (apply_threshold in code cell 16).
I used `scipy.ndimage.measurements.label()` to identify individual blobs in the thresholded heat map (code cell 18). I then assumed each blob corresponded to a vehicle.  I constructed bounding boxes to cover the area of each blob detected (draw_labeled_bboxes function in code cell 16).
For processing video images all this functionality was bundled into the process_image function (code cell 17).

Here's an example result showing the heat map from a series of frames of video, the result of `scipy.ndimage.measurements.label()` and the bounding boxes then overlaid on the last frame of video:

### Here are six frames and their corresponding heatmaps:
![alt text][image5]

### Here is the output of `scipy.ndimage.measurements.label()` on the integrated heatmap from all six frames:
![alt text][image6]

### Here the resulting bounding boxes are drawn onto the last frame in the series:
![alt text][image7]

---
### Discussion

#### Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?

Overall this pipeline has a lot of moving parts that have to be optimized simultaneously, which increases the chance to get stuck in a local optima.  

The detection of vehicles will probably get much harder in heavy traffic, with a lot of overlapping vehicles.
All conditions that make it harder to differentiate the cars from the surroundings like black cars in the night or white cars in a snow covered landscape will be problematic as well.

The results improve significantly by averaging over multiple frames, but the optimization is unique to the project clip and might not work so well on other videos. Furthermore I did not investigate all the possible color spaces for feature extraction. There might by improvement opportunities there as well. Additional sensor data obviously has potential to improve the pipeline.
